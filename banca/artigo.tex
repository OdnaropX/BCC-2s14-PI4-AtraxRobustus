\documentclass[12pt]{article}
\usepackage{sbc-template}
\usepackage{graphicx,url}
\usepackage[brazil]{babel}   
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{amsmath}
\usepackage{bbm}
\usepackage{cite}
\usepackage{listings}
\usepackage{color}
\usepackage{indentfirst}
\usepackage{float}

\lstset{language=Python,
                basicstyle=\ttfamily,
                keywordstyle=\color{blue}\ttfamily,
                stringstyle=\color{red}\ttfamily,
                commentstyle=\color{green}\ttfamily,
                morecomment=[l][\color{magenta}]{\#}
}


\title{Visualização da Dados de Informações Extraídas da Web - Um estudo sobre Conteúdo Popular Japonês}
\author{Gabriel Fontenelle\inst{1}}
\address 
{Centro Universitário Senac - Campus Santo Amaro
  (SENAC-SP)\\
  Av. Engenheiro Eusébio Stevaux, 823 -- São Paulo -- CEP 04696-000 -- SP -- Brasil
%\nextinstitute
%  Departamento de Tecnologia da Informação\\
%  Bacharelado em Ciência da Computação
  \email{{colecionador.gabriel@gmail.com}}
}

\begin{document} 
\maketitle

\begin{abstract}

This work 
\end{abstract}
     
\begin{resumo} 


Este trabalho apresenta um estudo sobre coleções de itens da cultura popular japonesa na forma de visualização de dados obtidos por meio de crawleamento de diversos websites.

\end{resumo}


\section{Introdução}

O foco deste trabalho é a criação de Visualização de dados a partir de informações extraídas de websites que disponibilizam quantidade de dados maciça.

Para a criação da Visualização de Dados foi escolhido o tema: Cultura Popular Japonesa. O tema é abrangente e poderíamos obter e utilizar informações sobre os seus diversos produtos comercializados: mangás, animes, Light Novels, Visual Novels e outros bens de consumo derivados como braceletes e "Figuras de Ação" que são colecionados por colecionadores em grande parte do mundo ocidental.

A Cultura Popular Japonesa é conhecida pelo desenvolvimento de animações, revistas em quadrinhos e gêneros literários influenciados por um estilo de desenho único focado nas expressões de suas personagens. Nos países ocidentais as animações, histórias em quadrinhos originadas no Japão são nomeadas respectivamente como anime e manga.
Com a popularização de jogos eletrônicos surgiram dois gêneros literários no Japão: Light Novel e Visual Novel. Light Novel é um gênero literário caracterizado pelo menor número de páginas e escrita mais clara utilizando-se em maior parte de Furiganas, possui histórias fluídas com desenvolvimento rápido muitas vezes utilizando-se de efeitos sonoros para ilustrar situações em vez de uma descrição completa da situação, como exemplo a saída de uma personagem de um quarto com uma batida grosseira da porta pode demonstrada apenas mencionando que a personagem saiu o efeito sonoro realizado na porta. Light Novel também é caracterizado pelas ilustrações de algumas cenas e por isso pode ser comparado a Graphic Novels (Histórias em inglês que possuem algumas páginas ilustradas). Visual Novel também é formado por ilustrações e textos mais claros de serem entendidos que livros tradicionais, mas diferente de Light Novel se assemelham mais a um jogo de computador em que decisões dos jogadores podem alterar o rumo da história.

Neste trabalho as informações foram extraídas de diversos websites com o uso de biblioteca de Crawler e salvas em um banco de dados previamente modelado para geração posterior de diversas visualizações de dados. 

Visualização de dados é uma forma de comunicar visualmente informações que de outro modo não seriam facilmente identificáveis.


\section{Desenvolvimento}

\subsection{Websites para extração de dados}

O tema escolhido é abrangente assim como possui diversos websites que poderíamos utilizar para a extração de informações. Focamo-nos na visualização de dados ilustrando informações sobre Coleções e seus produtos derivados como artigos decorativos e "Figuras de Ação" baseados em personagens presentes nas Coleções. 

Os websites escolhidos se encontram abaixo, a escolha de websites com informações reduntantes é proposital uma vez que não poderíamos garantir a conectividade com os sites durante o desenvolvimento deste trabalho. 

\begin{enumerate}

\end{enumerate}

A maioria dos sites escolhidos possuem informações no idioma inglês mas também no idioma Japonês, como títulos de mangás e animes. Para aproveitar essas informações o Banco de Dados foi modelado a permitir múltiplos idiomas. 

\subsection{Modelagem do Banco de Dados}

Antes sequer de prosseguirmos com o desenvolvimento do sistema de extração de informações dos websites optamos por criar um Banco de Dados normalizado para quando fossemos extrair e salvar as informações desses websites, os dados já seriam salvos em uma estrutura relacionado e normalizado.

Foi portanto desenvolvido o Modelo Conceitual, Modelo Relacional e Modelo Lógico do Banco de Dados.

Para o Sistema de Banco de Dados foi escolhido o PostgreSQL, sob licença BSD, disponível para diversos sistemas operacionais, possuindo Transações que seguem o modelo ACID com recursos como Commit e Rollback e opção para criação de atributos com tipos de dados personalizados. 

Este trabalho faz uso extenso de Commit e Rollback no tratamento de erros, assim transações com falhas parciais são descartadas evitando o salvamento de conteúdo incompleto.

%Como para extração dos dados dos websites escolhidos é necessário 
%Com a mentalidade que é melhor pegar todas as informações disponibilizadas pois é melhor ter que precisar e não ter, foi modelado um Banco de Dados que expandiu o conceito original e possibilita a inserção de dados relacionais para armazenamento de músicas, vídeos, livros e diversas informações associadas. 

\subsubsection{Modelo Conceitual}

O conceito inicial de armazenar informações relacionais sobre itens da Cultura Popular Japonesa resultou em um Modelo Conceitual em que se poderia armazenar informações de mangás, animes e "Figuras de Ação". Esse conceito original foi expandido para possibilitar a inserção de dados relacionais sobre músicas, jogos de computador, vídeos, livros e diversas informações associadas a Coleções. 

Com a expansão do tipo de informação a ser salva no Banco de Dados a criação do Modelo Conceitual se tornou complexa, além do tradicional uso de atributos e relacionamentos foram utilizados conceito de especialização/generalização e associação. 

Como a estrutura do Modelo Conceitual é complexa e grande dividimos sua ilustração em partes e para melhor compreensão antes de detalharmos cada parte apresentaremos um diagrama mais simples abrangendo as principais entidades do Modelo Conceitual:

\begin{figure}[H]
\centering
\includegraphics[width=.80\textwidth]{Resumo_Conceitual.pdf}
\caption{Modelo Conceitual resumido com os nomes das principais entidades em português. Na implementação do Modelo Lógico foram utilizados nomes em inglês.} \label{hash}
\end{figure}





os principais recursossua organização mais árdua, portanto . No modelo além do tradicional uso de atributos e relacionamentos foram utilizados conceito de especialização/generalização e associação.

Como exemplo de especialização/generalização temos no Modelo Conceitual a entitidade Goods onde bens de consumo como braceletes, posters, chaveiros são armazenados. Essa entidade se especializa na entidade Figure que armazena informações de bens conhecidos como figures ou action figures, são artigos colecionáveis em Três Dimensões de Personagens de animes, mangás, video games e light novels,
 que possuem informações extras além das contidas em Goods como escala e versão de lançamento.

Imagem da especialização.

Como exemplo de Associação temos o relacionamento entre pessoas que dublam animações, animações dubladas e edições dubladas que são associadas ao número da edição, para situações onde o dublador oficial não pode dublar um episodio ou uma lista de episodios por motivos fora de seu controle. 



\subsubsection{Erros na escolha das ferramentas}

Para a modelagem da Banco de Dados foi escolhido o Microsoft Visio que permite a geração de diversos tipos de diagramas, possibilitando a criação de novos itens se necessários para uso nos diagramas. Alguns dos itens como Associação, Atributos Compostos não estão presentes por padrão na modelagem de dados que vem com o Microsoft Visio e tiveram que serem criadas.

Para o Modelo Relacional foi utilizado o DBDesign na sua 4ª versão, que permite a migração do Modelo Relacional para código SQL do MySQL, porém o DBDesign se mostrou limitado ao não permitir o aumento na área útil em que as entidades poderiam ser adicionadas, para uma modelagem extensa como a do Banco de Dados desse projeto essa limitação dificultou a organização pela falta de espaço fazendo com que as entidades ficassem muito próximas umas das outras. 
Além dessa limitação a própria implementação em JAVA do DB Design apresenta problemas com a hibernação no Sistema Operacional Windows, ao se hibernar e retornar quando se tenta salvar ou exportar um arquivo no DBDesign o programa trava sendo necessário reniciar o computador para voltar a utilizar essas opções.

Portanto não recomendo o uso do DB Design para um modelo relacional extenso. 

A conversão de SQL do MySQL para PostgreSQL não apresenta problemas, sendo necessário usar SERIAL e timestamp ao invés de utilizar respectivamente $auto_increment$ e DATETIME


\subsection{Crawler}

Um crawler é um programa que visitas websites segue seus links e extraí informações das páginas. Um crawler deve a partir de uma URL inicial seguir todas as URL presentes na página a fim de vasculhar o website, 
pode ser utilizados critérios para seguir essas URL, como por exemplo seguir URL apenas do domínio atual do site, excluindo websites externos de publicidade e redes sociais.

O Crawler na URL inicial faz o download do código HTML da página e extraí a informação "href" do atributo <a> e os coloca numa pilha de processamento que será utilizada para analisar se o link deve ser seguido ou removido da pilha. 
Para evitar o problema de loops infinitos quando uma página A possui um link para uma página B e a página B possui link para a página A, deve-se criar uma pilha de links já visitados para verificação.   
 
Neste projeto foi optado pelo uso de uma biblioteca de Crawler que permite o seguimento de URL de páginas e extração de informações sem ter que se preocupar em programar Request e Download de páginas da Web.

O Biblioteca escolhida foi o Scrapy, na versão 0.24.4, desenvolvida em Python, possibilitando a criação da lógica de links a serem seguidos usando regras conhecidas como Rules que salvam informações em expressão regular das características dos links a serem visitados ou a serem removidos da pilha de processamento.
 
Como os websites a serem utilizados possuem conteúdo estático sem uso de Ajax o Scrapy atende as necessidades básicas de download e parseamento de websites. Se for necessário extrair informações dinamicamente geradas por AJAX pode ser utilizado o Scrapy com extensão para Firefox ou outra biblioteca de Crawler que permite o download do código-fonte da página como visualizado pelos browser, como a biblioteca Selenium.

Scrapy assim como Python está disponível para Windows e Linux.


\subsubsection{Implementando acesso ao Banco de Dados pela biblioteca Psycopg2}


\subsubsection{Cortando Atalhos - Uso de Transaction}


\subsection{Normalização dos Dados Salvos}

Após extrair os dados, esses foram analisados e salvos normalizados. Porém algumas informações só podem ser estipuladas após a obtenção de todos os dados.

Como exemplo temos o item que iniciou uma coleção, só pode ser estipulado a partir da data de lançamento dos itens integrantes da coleçao e só após todos os dados serem salvos podemos descobrir qual item é o mais antigo da coleção.




\subsubsection{1º Passo no Desenvolvimento}

Com a escolha da biblioteca para o crawler foram analisados os links das páginas escolhidas e definidos quais links seriam seguidos em busca de outros links para conteúdo assim como quais links seriam de conteúdo para parseamento. 



\subsubsection{Links Duplicados}

Quando se desenvolve um crawler, ou seja, um algoritmo para visitação de links presentes em páginas da web se deve evitar a visita de links já visitados, característica causadora de loops inesperados. 

Há muitas formas de se fazer um crawler com a biblioteca Scrapy e utilizamos a classe CrawlSpider que permiti uma implementação rápida que lida com links duplicados, recurso já implementado na classe.


\subsubsection {Abordagens de Spiders}

Abordagem assincrona.

Sobreescrita de métodos para efetuar login, de métodos
Implementação de Requests na própria classe requer um tratamento diferenciado ao código, só pode existir uma chamada a Request no CrawlSpider, mais que isso e o CrawlSpider não consegue lidar com request e interrompe a busca =.



\subsection{Processamento e relacionamento de Dados}

Após a extração dos dados e inserção no Banco de dados, os dados precisariam ser ....


\subsection{Visualização de Dados}

\section{Resultados}

Resultados.
Quais foram os dados considerados? Qual é o volume desses dados? Quanto tempo foi
necessário para obtê-los e armazená-los? Quais visualizações foram consideradas interessantes e quais não
foram? Quanto tempo foi necessário para gerar essas visualizações? Alguma delas foi surpreendente?


\section{Conclusão}


\bibliographystyle{plain}
\begin{thebibliography}{2}

\end{thebibliography}
\end{document}