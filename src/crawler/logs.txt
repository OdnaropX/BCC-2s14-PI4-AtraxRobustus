2014-10-07 14:45:12-0300 [scrapy] INFO: Scrapy 0.24.4 started (bot: crawler)
2014-10-07 14:45:12-0300 [scrapy] INFO: Optional features available: ssl, http11
2014-10-07 14:45:12-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'crawler.spiders', 'SPIDER_MODULES': ['crawler.spiders'], 'LOG_FILE': 'logs.txt', 'BOT_NAME': 'crawler'}
2014-10-07 14:45:13-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2014-10-07 14:45:14-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2014-10-07 14:45:14-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2014-10-07 14:45:14-0300 [scrapy] INFO: Enabled item pipelines: 
2014-10-07 14:45:14-0300 [mangaupdates] INFO: Spider opened
2014-10-07 14:45:14-0300 [mangaupdates] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2014-10-07 14:45:14-0300 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2014-10-07 14:45:14-0300 [scrapy] DEBUG: Web service listening on 127.0.0.1:6080
2014-10-07 14:45:14-0300 [mangaupdates] DEBUG: Crawled (200) <GET http://www.mangaupdates.com/publishers.html?page=1&> (referer: None)
2014-10-07 14:45:14-0300 [mangaupdates] ERROR: Spider error processing <GET http://www.mangaupdates.com/publishers.html?page=1&>
	Traceback (most recent call last):
	  File "c:\python27\lib\site-packages\twisted\internet\base.py", line 1201, in mainLoop
	    self.runUntilCurrent()
	  File "c:\python27\lib\site-packages\twisted\internet\base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "c:\python27\lib\site-packages\twisted\internet\defer.py", line 382, in callback
	    self._startRunCallbacks(result)
	  File "c:\python27\lib\site-packages\twisted\internet\defer.py", line 490, in _startRunCallbacks
	    self._runCallbacks()
	--- <exception caught here> ---
	  File "c:\python27\lib\site-packages\twisted\internet\defer.py", line 577, in _runCallbacks
	    current.result = callback(current.result, *args, **kw)
	  File "J:\Documentos\Senac\Quarto Semestre\Projeto Interativo IV\BCC-2s14-PI4-AtraxRobustus\src\crawler\crawler\spiders\mangaupdates.py", line 50, in parse
	    instancialize_database()
	exceptions.NameError: global name 'instancialize_database' is not defined
	
2014-10-07 14:45:14-0300 [mangaupdates] DEBUG: Crawled (200) <GET http://www.mangaupdates.com/groups.html?page=1&> (referer: None)
2014-10-07 14:45:14-0300 [mangaupdates] DEBUG: Crawled (200) <GET http://www.mangaupdates.com/authors.html?page=1&> (referer: None)
2014-10-07 14:45:14-0300 [mangaupdates] ERROR: Spider error processing <GET http://www.mangaupdates.com/groups.html?page=1&>
	Traceback (most recent call last):
	  File "c:\python27\lib\site-packages\twisted\internet\base.py", line 1201, in mainLoop
	    self.runUntilCurrent()
	  File "c:\python27\lib\site-packages\twisted\internet\base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "c:\python27\lib\site-packages\twisted\internet\defer.py", line 382, in callback
	    self._startRunCallbacks(result)
	  File "c:\python27\lib\site-packages\twisted\internet\defer.py", line 490, in _startRunCallbacks
	    self._runCallbacks()
	--- <exception caught here> ---
	  File "c:\python27\lib\site-packages\twisted\internet\defer.py", line 577, in _runCallbacks
	    current.result = callback(current.result, *args, **kw)
	  File "J:\Documentos\Senac\Quarto Semestre\Projeto Interativo IV\BCC-2s14-PI4-AtraxRobustus\src\crawler\crawler\spiders\mangaupdates.py", line 50, in parse
	    instancialize_database()
	exceptions.NameError: global name 'instancialize_database' is not defined
	
2014-10-07 14:45:14-0300 [mangaupdates] ERROR: Spider error processing <GET http://www.mangaupdates.com/authors.html?page=1&>
	Traceback (most recent call last):
	  File "c:\python27\lib\site-packages\twisted\internet\base.py", line 1201, in mainLoop
	    self.runUntilCurrent()
	  File "c:\python27\lib\site-packages\twisted\internet\base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "c:\python27\lib\site-packages\twisted\internet\defer.py", line 382, in callback
	    self._startRunCallbacks(result)
	  File "c:\python27\lib\site-packages\twisted\internet\defer.py", line 490, in _startRunCallbacks
	    self._runCallbacks()
	--- <exception caught here> ---
	  File "c:\python27\lib\site-packages\twisted\internet\defer.py", line 577, in _runCallbacks
	    current.result = callback(current.result, *args, **kw)
	  File "J:\Documentos\Senac\Quarto Semestre\Projeto Interativo IV\BCC-2s14-PI4-AtraxRobustus\src\crawler\crawler\spiders\mangaupdates.py", line 50, in parse
	    instancialize_database()
	exceptions.NameError: global name 'instancialize_database' is not defined
	
2014-10-07 14:45:14-0300 [mangaupdates] DEBUG: Crawled (200) <GET http://www.mangaupdates.com/series.html?page=1&> (referer: None)
2014-10-07 14:45:14-0300 [mangaupdates] ERROR: Spider error processing <GET http://www.mangaupdates.com/series.html?page=1&>
	Traceback (most recent call last):
	  File "c:\python27\lib\site-packages\twisted\internet\base.py", line 1201, in mainLoop
	    self.runUntilCurrent()
	  File "c:\python27\lib\site-packages\twisted\internet\base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "c:\python27\lib\site-packages\twisted\internet\defer.py", line 382, in callback
	    self._startRunCallbacks(result)
	  File "c:\python27\lib\site-packages\twisted\internet\defer.py", line 490, in _startRunCallbacks
	    self._runCallbacks()
	--- <exception caught here> ---
	  File "c:\python27\lib\site-packages\twisted\internet\defer.py", line 577, in _runCallbacks
	    current.result = callback(current.result, *args, **kw)
	  File "J:\Documentos\Senac\Quarto Semestre\Projeto Interativo IV\BCC-2s14-PI4-AtraxRobustus\src\crawler\crawler\spiders\mangaupdates.py", line 50, in parse
	    instancialize_database()
	exceptions.NameError: global name 'instancialize_database' is not defined
	
2014-10-07 14:45:14-0300 [mangaupdates] DEBUG: Crawled (200) <GET http://www.mangaupdates.com/categories.html?page=1&> (referer: None)
2014-10-07 14:45:14-0300 [mangaupdates] ERROR: Spider error processing <GET http://www.mangaupdates.com/categories.html?page=1&>
	Traceback (most recent call last):
	  File "c:\python27\lib\site-packages\twisted\internet\base.py", line 1201, in mainLoop
	    self.runUntilCurrent()
	  File "c:\python27\lib\site-packages\twisted\internet\base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "c:\python27\lib\site-packages\twisted\internet\defer.py", line 382, in callback
	    self._startRunCallbacks(result)
	  File "c:\python27\lib\site-packages\twisted\internet\defer.py", line 490, in _startRunCallbacks
	    self._runCallbacks()
	--- <exception caught here> ---
	  File "c:\python27\lib\site-packages\twisted\internet\defer.py", line 577, in _runCallbacks
	    current.result = callback(current.result, *args, **kw)
	  File "J:\Documentos\Senac\Quarto Semestre\Projeto Interativo IV\BCC-2s14-PI4-AtraxRobustus\src\crawler\crawler\spiders\mangaupdates.py", line 50, in parse
	    instancialize_database()
	exceptions.NameError: global name 'instancialize_database' is not defined
	
2014-10-07 14:45:15-0300 [mangaupdates] DEBUG: Crawled (200) <GET http://www.mangaupdates.com/genres.html> (referer: None)
2014-10-07 14:45:15-0300 [mangaupdates] ERROR: Spider error processing <GET http://www.mangaupdates.com/genres.html>
	Traceback (most recent call last):
	  File "c:\python27\lib\site-packages\twisted\internet\base.py", line 1201, in mainLoop
	    self.runUntilCurrent()
	  File "c:\python27\lib\site-packages\twisted\internet\base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "c:\python27\lib\site-packages\twisted\internet\defer.py", line 382, in callback
	    self._startRunCallbacks(result)
	  File "c:\python27\lib\site-packages\twisted\internet\defer.py", line 490, in _startRunCallbacks
	    self._runCallbacks()
	--- <exception caught here> ---
	  File "c:\python27\lib\site-packages\twisted\internet\defer.py", line 577, in _runCallbacks
	    current.result = callback(current.result, *args, **kw)
	  File "J:\Documentos\Senac\Quarto Semestre\Projeto Interativo IV\BCC-2s14-PI4-AtraxRobustus\src\crawler\crawler\spiders\mangaupdates.py", line 50, in parse
	    instancialize_database()
	exceptions.NameError: global name 'instancialize_database' is not defined
	
2014-10-07 14:45:15-0300 [mangaupdates] INFO: Closing spider (finished)
2014-10-07 14:45:15-0300 [mangaupdates] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 1429,
	 'downloader/request_count': 6,
	 'downloader/request_method_count/GET': 6,
	 'downloader/response_bytes': 32941,
	 'downloader/response_count': 6,
	 'downloader/response_status_count/200': 6,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2014, 10, 7, 17, 45, 15, 172000),
	 'log_count/DEBUG': 8,
	 'log_count/ERROR': 6,
	 'log_count/INFO': 7,
	 'response_received_count': 6,
	 'scheduler/dequeued': 6,
	 'scheduler/dequeued/memory': 6,
	 'scheduler/enqueued': 6,
	 'scheduler/enqueued/memory': 6,
	 'spider_exceptions/NameError': 6,
	 'start_time': datetime.datetime(2014, 10, 7, 17, 45, 14, 40000)}
2014-10-07 14:45:15-0300 [mangaupdates] INFO: Spider closed (finished)
2014-10-07 15:21:54-0300 [scrapy] INFO: Scrapy 0.24.4 started (bot: crawler)
2014-10-07 15:21:54-0300 [scrapy] INFO: Optional features available: ssl, http11
2014-10-07 15:21:54-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'crawler.spiders', 'SPIDER_MODULES': ['crawler.spiders'], 'LOG_FILE': 'logs.txt', 'BOT_NAME': 'crawler'}
2014-10-07 15:21:55-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2014-10-07 15:21:55-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2014-10-07 15:21:55-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2014-10-07 15:21:55-0300 [scrapy] INFO: Enabled item pipelines: 
2014-10-07 15:21:55-0300 [mangaupdates] INFO: Spider opened
2014-10-07 15:21:55-0300 [mangaupdates] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2014-10-07 15:21:55-0300 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2014-10-07 15:21:55-0300 [scrapy] DEBUG: Web service listening on 127.0.0.1:6080
2014-10-07 15:21:56-0300 [mangaupdates] DEBUG: Crawled (200) <GET http://www.mangaupdates.com/categories.html?page=1&> (referer: None)
2014-10-07 15:21:56-0300 [mangaupdates] ERROR: Spider error processing <GET http://www.mangaupdates.com/categories.html?page=1&>
	Traceback (most recent call last):
	  File "c:\python27\lib\site-packages\twisted\internet\base.py", line 1201, in mainLoop
	    self.runUntilCurrent()
	  File "c:\python27\lib\site-packages\twisted\internet\base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "c:\python27\lib\site-packages\twisted\internet\defer.py", line 382, in callback
	    self._startRunCallbacks(result)
	  File "c:\python27\lib\site-packages\twisted\internet\defer.py", line 490, in _startRunCallbacks
	    self._runCallbacks()
	--- <exception caught here> ---
	  File "c:\python27\lib\site-packages\twisted\internet\defer.py", line 577, in _runCallbacks
	    current.result = callback(current.result, *args, **kw)
	  File "J:\Documentos\Senac\Quarto Semestre\Projeto Interativo IV\BCC-2s14-PI4-AtraxRobustus\src\crawler\crawler\spiders\mangaupdates.py", line 53, in parse
	    self.dbase.insert('function_type', ['teste'], ['name'])
	  File "J:\Documentos\Senac\Quarto Semestre\Projeto Interativo IV\BCC-2s14-PI4-AtraxRobustus\src\crawler\crawler\database.py", line 114, in insert
	    sql = "INSERT INTO {table} ({columns}) VALUES ({values});".format(table=table, columns = columns.join(","), values=value.join(","))
	exceptions.AttributeError: 'list' object has no attribute 'join'
	
2014-10-07 15:21:56-0300 [mangaupdates] DEBUG: Crawled (200) <GET http://www.mangaupdates.com/authors.html?page=1&> (referer: None)
2014-10-07 15:21:56-0300 [mangaupdates] DEBUG: Crawled (200) <GET http://www.mangaupdates.com/publishers.html?page=1&> (referer: None)
2014-10-07 15:21:56-0300 [mangaupdates] DEBUG: Crawled (200) <GET http://www.mangaupdates.com/groups.html?page=1&> (referer: None)
2014-10-07 15:21:56-0300 [mangaupdates] ERROR: Spider error processing <GET http://www.mangaupdates.com/authors.html?page=1&>
	Traceback (most recent call last):
	  File "c:\python27\lib\site-packages\twisted\internet\base.py", line 1201, in mainLoop
	    self.runUntilCurrent()
	  File "c:\python27\lib\site-packages\twisted\internet\base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "c:\python27\lib\site-packages\twisted\internet\defer.py", line 382, in callback
	    self._startRunCallbacks(result)
	  File "c:\python27\lib\site-packages\twisted\internet\defer.py", line 490, in _startRunCallbacks
	    self._runCallbacks()
	--- <exception caught here> ---
	  File "c:\python27\lib\site-packages\twisted\internet\defer.py", line 577, in _runCallbacks
	    current.result = callback(current.result, *args, **kw)
	  File "J:\Documentos\Senac\Quarto Semestre\Projeto Interativo IV\BCC-2s14-PI4-AtraxRobustus\src\crawler\crawler\spiders\mangaupdates.py", line 53, in parse
	    self.dbase.insert('function_type', ['teste'], ['name'])
	  File "J:\Documentos\Senac\Quarto Semestre\Projeto Interativo IV\BCC-2s14-PI4-AtraxRobustus\src\crawler\crawler\database.py", line 114, in insert
	    sql = "INSERT INTO {table} ({columns}) VALUES ({values});".format(table=table, columns = columns.join(","), values=value.join(","))
	exceptions.AttributeError: 'list' object has no attribute 'join'
	
2014-10-07 15:21:56-0300 [mangaupdates] ERROR: Spider error processing <GET http://www.mangaupdates.com/publishers.html?page=1&>
	Traceback (most recent call last):
	  File "c:\python27\lib\site-packages\twisted\internet\base.py", line 1201, in mainLoop
	    self.runUntilCurrent()
	  File "c:\python27\lib\site-packages\twisted\internet\base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "c:\python27\lib\site-packages\twisted\internet\defer.py", line 382, in callback
	    self._startRunCallbacks(result)
	  File "c:\python27\lib\site-packages\twisted\internet\defer.py", line 490, in _startRunCallbacks
	    self._runCallbacks()
	--- <exception caught here> ---
	  File "c:\python27\lib\site-packages\twisted\internet\defer.py", line 577, in _runCallbacks
	    current.result = callback(current.result, *args, **kw)
	  File "J:\Documentos\Senac\Quarto Semestre\Projeto Interativo IV\BCC-2s14-PI4-AtraxRobustus\src\crawler\crawler\spiders\mangaupdates.py", line 53, in parse
	    self.dbase.insert('function_type', ['teste'], ['name'])
	  File "J:\Documentos\Senac\Quarto Semestre\Projeto Interativo IV\BCC-2s14-PI4-AtraxRobustus\src\crawler\crawler\database.py", line 114, in insert
	    sql = "INSERT INTO {table} ({columns}) VALUES ({values});".format(table=table, columns = columns.join(","), values=value.join(","))
	exceptions.AttributeError: 'list' object has no attribute 'join'
	
2014-10-07 15:21:56-0300 [mangaupdates] ERROR: Spider error processing <GET http://www.mangaupdates.com/groups.html?page=1&>
	Traceback (most recent call last):
	  File "c:\python27\lib\site-packages\twisted\internet\base.py", line 1201, in mainLoop
	    self.runUntilCurrent()
	  File "c:\python27\lib\site-packages\twisted\internet\base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "c:\python27\lib\site-packages\twisted\internet\defer.py", line 382, in callback
	    self._startRunCallbacks(result)
	  File "c:\python27\lib\site-packages\twisted\internet\defer.py", line 490, in _startRunCallbacks
	    self._runCallbacks()
	--- <exception caught here> ---
	  File "c:\python27\lib\site-packages\twisted\internet\defer.py", line 577, in _runCallbacks
	    current.result = callback(current.result, *args, **kw)
	  File "J:\Documentos\Senac\Quarto Semestre\Projeto Interativo IV\BCC-2s14-PI4-AtraxRobustus\src\crawler\crawler\spiders\mangaupdates.py", line 53, in parse
	    self.dbase.insert('function_type', ['teste'], ['name'])
	  File "J:\Documentos\Senac\Quarto Semestre\Projeto Interativo IV\BCC-2s14-PI4-AtraxRobustus\src\crawler\crawler\database.py", line 114, in insert
	    sql = "INSERT INTO {table} ({columns}) VALUES ({values});".format(table=table, columns = columns.join(","), values=value.join(","))
	exceptions.AttributeError: 'list' object has no attribute 'join'
	
2014-10-07 15:21:56-0300 [mangaupdates] DEBUG: Crawled (200) <GET http://www.mangaupdates.com/series.html?page=1&> (referer: None)
2014-10-07 15:21:56-0300 [mangaupdates] ERROR: Spider error processing <GET http://www.mangaupdates.com/series.html?page=1&>
	Traceback (most recent call last):
	  File "c:\python27\lib\site-packages\twisted\internet\base.py", line 1201, in mainLoop
	    self.runUntilCurrent()
	  File "c:\python27\lib\site-packages\twisted\internet\base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "c:\python27\lib\site-packages\twisted\internet\defer.py", line 382, in callback
	    self._startRunCallbacks(result)
	  File "c:\python27\lib\site-packages\twisted\internet\defer.py", line 490, in _startRunCallbacks
	    self._runCallbacks()
	--- <exception caught here> ---
	  File "c:\python27\lib\site-packages\twisted\internet\defer.py", line 577, in _runCallbacks
	    current.result = callback(current.result, *args, **kw)
	  File "J:\Documentos\Senac\Quarto Semestre\Projeto Interativo IV\BCC-2s14-PI4-AtraxRobustus\src\crawler\crawler\spiders\mangaupdates.py", line 53, in parse
	    self.dbase.insert('function_type', ['teste'], ['name'])
	  File "J:\Documentos\Senac\Quarto Semestre\Projeto Interativo IV\BCC-2s14-PI4-AtraxRobustus\src\crawler\crawler\database.py", line 114, in insert
	    sql = "INSERT INTO {table} ({columns}) VALUES ({values});".format(table=table, columns = columns.join(","), values=value.join(","))
	exceptions.AttributeError: 'list' object has no attribute 'join'
	
2014-10-07 15:21:57-0300 [mangaupdates] DEBUG: Crawled (200) <GET http://www.mangaupdates.com/genres.html> (referer: None)
2014-10-07 15:21:57-0300 [mangaupdates] ERROR: Spider error processing <GET http://www.mangaupdates.com/genres.html>
	Traceback (most recent call last):
	  File "c:\python27\lib\site-packages\twisted\internet\base.py", line 1201, in mainLoop
	    self.runUntilCurrent()
	  File "c:\python27\lib\site-packages\twisted\internet\base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "c:\python27\lib\site-packages\twisted\internet\defer.py", line 382, in callback
	    self._startRunCallbacks(result)
	  File "c:\python27\lib\site-packages\twisted\internet\defer.py", line 490, in _startRunCallbacks
	    self._runCallbacks()
	--- <exception caught here> ---
	  File "c:\python27\lib\site-packages\twisted\internet\defer.py", line 577, in _runCallbacks
	    current.result = callback(current.result, *args, **kw)
	  File "J:\Documentos\Senac\Quarto Semestre\Projeto Interativo IV\BCC-2s14-PI4-AtraxRobustus\src\crawler\crawler\spiders\mangaupdates.py", line 53, in parse
	    self.dbase.insert('function_type', ['teste'], ['name'])
	  File "J:\Documentos\Senac\Quarto Semestre\Projeto Interativo IV\BCC-2s14-PI4-AtraxRobustus\src\crawler\crawler\database.py", line 114, in insert
	    sql = "INSERT INTO {table} ({columns}) VALUES ({values});".format(table=table, columns = columns.join(","), values=value.join(","))
	exceptions.AttributeError: 'list' object has no attribute 'join'
	
2014-10-07 15:21:57-0300 [mangaupdates] INFO: Closing spider (finished)
2014-10-07 15:21:57-0300 [mangaupdates] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 1429,
	 'downloader/request_count': 6,
	 'downloader/request_method_count/GET': 6,
	 'downloader/response_bytes': 32939,
	 'downloader/response_count': 6,
	 'downloader/response_status_count/200': 6,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2014, 10, 7, 18, 21, 57, 112000),
	 'log_count/DEBUG': 8,
	 'log_count/ERROR': 6,
	 'log_count/INFO': 7,
	 'response_received_count': 6,
	 'scheduler/dequeued': 6,
	 'scheduler/dequeued/memory': 6,
	 'scheduler/enqueued': 6,
	 'scheduler/enqueued/memory': 6,
	 'spider_exceptions/AttributeError': 6,
	 'start_time': datetime.datetime(2014, 10, 7, 18, 21, 55, 913000)}
2014-10-07 15:21:57-0300 [mangaupdates] INFO: Spider closed (finished)
2014-10-07 20:01:52-0300 [scrapy] INFO: Scrapy 0.24.4 started (bot: crawler)
2014-10-07 20:01:52-0300 [scrapy] INFO: Optional features available: ssl, http11
2014-10-07 20:01:52-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'crawler.spiders', 'SPIDER_MODULES': ['crawler.spiders'], 'LOG_FILE': 'logs.txt', 'BOT_NAME': 'crawler'}
2014-10-07 20:01:53-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2014-10-07 20:01:53-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2014-10-07 20:01:53-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2014-10-07 20:01:53-0300 [scrapy] INFO: Enabled item pipelines: 
2014-10-07 20:01:53-0300 [mangaupdates] INFO: Spider opened
2014-10-07 20:01:53-0300 [mangaupdates] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2014-10-07 20:01:53-0300 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2014-10-07 20:01:53-0300 [scrapy] DEBUG: Web service listening on 127.0.0.1:6080
2014-10-07 20:01:54-0300 [mangaupdates] DEBUG: Crawled (200) <GET http://www.mangaupdates.com/groups.html?page=1&> (referer: None)
2014-10-07 20:01:54-0300 [mangaupdates] ERROR: Spider error processing <GET http://www.mangaupdates.com/groups.html?page=1&>
	Traceback (most recent call last):
	  File "c:\python27\lib\site-packages\twisted\internet\base.py", line 1201, in mainLoop
	    self.runUntilCurrent()
	  File "c:\python27\lib\site-packages\twisted\internet\base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "c:\python27\lib\site-packages\twisted\internet\defer.py", line 382, in callback
	    self._startRunCallbacks(result)
	  File "c:\python27\lib\site-packages\twisted\internet\defer.py", line 490, in _startRunCallbacks
	    self._runCallbacks()
	--- <exception caught here> ---
	  File "c:\python27\lib\site-packages\twisted\internet\defer.py", line 577, in _runCallbacks
	    current.result = callback(current.result, *args, **kw)
	  File "J:\Documentos\Senac\Quarto Semestre\Projeto Interativo IV\BCC-2s14-PI4-AtraxRobustus\src\crawler\crawler\spiders\mangaupdates.py", line 53, in parse
	    self.dbase.insert('function_type', ['teste'], ['name'])
	  File "J:\Documentos\Senac\Quarto Semestre\Projeto Interativo IV\BCC-2s14-PI4-AtraxRobustus\src\crawler\crawler\database.py", line 129, in insert
	    self.insert_id = self._fetch_last_inserted_id(self, table, 'id')
	exceptions.TypeError: _fetch_last_inserted_id() takes exactly 3 arguments (4 given)
	
2014-10-07 20:01:54-0300 [mangaupdates] DEBUG: Crawled (200) <GET http://www.mangaupdates.com/authors.html?page=1&> (referer: None)
2014-10-07 20:01:54-0300 [mangaupdates] DEBUG: Crawled (200) <GET http://www.mangaupdates.com/publishers.html?page=1&> (referer: None)
2014-10-07 20:01:54-0300 [mangaupdates] DEBUG: Crawled (200) <GET http://www.mangaupdates.com/categories.html?page=1&> (referer: None)
2014-10-07 20:01:54-0300 [mangaupdates] ERROR: Spider error processing <GET http://www.mangaupdates.com/authors.html?page=1&>
	Traceback (most recent call last):
	  File "c:\python27\lib\site-packages\twisted\internet\base.py", line 1201, in mainLoop
	    self.runUntilCurrent()
	  File "c:\python27\lib\site-packages\twisted\internet\base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "c:\python27\lib\site-packages\twisted\internet\defer.py", line 382, in callback
	    self._startRunCallbacks(result)
	  File "c:\python27\lib\site-packages\twisted\internet\defer.py", line 490, in _startRunCallbacks
	    self._runCallbacks()
	--- <exception caught here> ---
	  File "c:\python27\lib\site-packages\twisted\internet\defer.py", line 577, in _runCallbacks
	    current.result = callback(current.result, *args, **kw)
	  File "J:\Documentos\Senac\Quarto Semestre\Projeto Interativo IV\BCC-2s14-PI4-AtraxRobustus\src\crawler\crawler\spiders\mangaupdates.py", line 53, in parse
	    self.dbase.insert('function_type', ['teste'], ['name'])
	  File "J:\Documentos\Senac\Quarto Semestre\Projeto Interativo IV\BCC-2s14-PI4-AtraxRobustus\src\crawler\crawler\database.py", line 129, in insert
	    self.insert_id = self._fetch_last_inserted_id(self, table, 'id')
	exceptions.TypeError: _fetch_last_inserted_id() takes exactly 3 arguments (4 given)
	
2014-10-07 20:01:54-0300 [mangaupdates] ERROR: Spider error processing <GET http://www.mangaupdates.com/publishers.html?page=1&>
	Traceback (most recent call last):
	  File "c:\python27\lib\site-packages\twisted\internet\base.py", line 1201, in mainLoop
	    self.runUntilCurrent()
	  File "c:\python27\lib\site-packages\twisted\internet\base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "c:\python27\lib\site-packages\twisted\internet\defer.py", line 382, in callback
	    self._startRunCallbacks(result)
	  File "c:\python27\lib\site-packages\twisted\internet\defer.py", line 490, in _startRunCallbacks
	    self._runCallbacks()
	--- <exception caught here> ---
	  File "c:\python27\lib\site-packages\twisted\internet\defer.py", line 577, in _runCallbacks
	    current.result = callback(current.result, *args, **kw)
	  File "J:\Documentos\Senac\Quarto Semestre\Projeto Interativo IV\BCC-2s14-PI4-AtraxRobustus\src\crawler\crawler\spiders\mangaupdates.py", line 53, in parse
	    self.dbase.insert('function_type', ['teste'], ['name'])
	  File "J:\Documentos\Senac\Quarto Semestre\Projeto Interativo IV\BCC-2s14-PI4-AtraxRobustus\src\crawler\crawler\database.py", line 129, in insert
	    self.insert_id = self._fetch_last_inserted_id(self, table, 'id')
	exceptions.TypeError: _fetch_last_inserted_id() takes exactly 3 arguments (4 given)
	
2014-10-07 20:01:54-0300 [mangaupdates] ERROR: Spider error processing <GET http://www.mangaupdates.com/categories.html?page=1&>
	Traceback (most recent call last):
	  File "c:\python27\lib\site-packages\twisted\internet\base.py", line 1201, in mainLoop
	    self.runUntilCurrent()
	  File "c:\python27\lib\site-packages\twisted\internet\base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "c:\python27\lib\site-packages\twisted\internet\defer.py", line 382, in callback
	    self._startRunCallbacks(result)
	  File "c:\python27\lib\site-packages\twisted\internet\defer.py", line 490, in _startRunCallbacks
	    self._runCallbacks()
	--- <exception caught here> ---
	  File "c:\python27\lib\site-packages\twisted\internet\defer.py", line 577, in _runCallbacks
	    current.result = callback(current.result, *args, **kw)
	  File "J:\Documentos\Senac\Quarto Semestre\Projeto Interativo IV\BCC-2s14-PI4-AtraxRobustus\src\crawler\crawler\spiders\mangaupdates.py", line 53, in parse
	    self.dbase.insert('function_type', ['teste'], ['name'])
	  File "J:\Documentos\Senac\Quarto Semestre\Projeto Interativo IV\BCC-2s14-PI4-AtraxRobustus\src\crawler\crawler\database.py", line 129, in insert
	    self.insert_id = self._fetch_last_inserted_id(self, table, 'id')
	exceptions.TypeError: _fetch_last_inserted_id() takes exactly 3 arguments (4 given)
	
2014-10-07 20:01:54-0300 [mangaupdates] DEBUG: Crawled (200) <GET http://www.mangaupdates.com/series.html?page=1&> (referer: None)
2014-10-07 20:01:54-0300 [mangaupdates] ERROR: Spider error processing <GET http://www.mangaupdates.com/series.html?page=1&>
	Traceback (most recent call last):
	  File "c:\python27\lib\site-packages\twisted\internet\base.py", line 1201, in mainLoop
	    self.runUntilCurrent()
	  File "c:\python27\lib\site-packages\twisted\internet\base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "c:\python27\lib\site-packages\twisted\internet\defer.py", line 382, in callback
	    self._startRunCallbacks(result)
	  File "c:\python27\lib\site-packages\twisted\internet\defer.py", line 490, in _startRunCallbacks
	    self._runCallbacks()
	--- <exception caught here> ---
	  File "c:\python27\lib\site-packages\twisted\internet\defer.py", line 577, in _runCallbacks
	    current.result = callback(current.result, *args, **kw)
	  File "J:\Documentos\Senac\Quarto Semestre\Projeto Interativo IV\BCC-2s14-PI4-AtraxRobustus\src\crawler\crawler\spiders\mangaupdates.py", line 53, in parse
	    self.dbase.insert('function_type', ['teste'], ['name'])
	  File "J:\Documentos\Senac\Quarto Semestre\Projeto Interativo IV\BCC-2s14-PI4-AtraxRobustus\src\crawler\crawler\database.py", line 129, in insert
	    self.insert_id = self._fetch_last_inserted_id(self, table, 'id')
	exceptions.TypeError: _fetch_last_inserted_id() takes exactly 3 arguments (4 given)
	
2014-10-07 20:01:55-0300 [mangaupdates] DEBUG: Crawled (200) <GET http://www.mangaupdates.com/genres.html> (referer: None)
2014-10-07 20:01:55-0300 [mangaupdates] ERROR: Spider error processing <GET http://www.mangaupdates.com/genres.html>
	Traceback (most recent call last):
	  File "c:\python27\lib\site-packages\twisted\internet\base.py", line 1201, in mainLoop
	    self.runUntilCurrent()
	  File "c:\python27\lib\site-packages\twisted\internet\base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "c:\python27\lib\site-packages\twisted\internet\defer.py", line 382, in callback
	    self._startRunCallbacks(result)
	  File "c:\python27\lib\site-packages\twisted\internet\defer.py", line 490, in _startRunCallbacks
	    self._runCallbacks()
	--- <exception caught here> ---
	  File "c:\python27\lib\site-packages\twisted\internet\defer.py", line 577, in _runCallbacks
	    current.result = callback(current.result, *args, **kw)
	  File "J:\Documentos\Senac\Quarto Semestre\Projeto Interativo IV\BCC-2s14-PI4-AtraxRobustus\src\crawler\crawler\spiders\mangaupdates.py", line 53, in parse
	    self.dbase.insert('function_type', ['teste'], ['name'])
	  File "J:\Documentos\Senac\Quarto Semestre\Projeto Interativo IV\BCC-2s14-PI4-AtraxRobustus\src\crawler\crawler\database.py", line 129, in insert
	    self.insert_id = self._fetch_last_inserted_id(self, table, 'id')
	exceptions.TypeError: _fetch_last_inserted_id() takes exactly 3 arguments (4 given)
	
2014-10-07 20:01:55-0300 [mangaupdates] INFO: Closing spider (finished)
2014-10-07 20:01:55-0300 [mangaupdates] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 1429,
	 'downloader/request_count': 6,
	 'downloader/request_method_count/GET': 6,
	 'downloader/response_bytes': 32929,
	 'downloader/response_count': 6,
	 'downloader/response_status_count/200': 6,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2014, 10, 7, 23, 1, 55, 141000),
	 'log_count/DEBUG': 8,
	 'log_count/ERROR': 6,
	 'log_count/INFO': 7,
	 'response_received_count': 6,
	 'scheduler/dequeued': 6,
	 'scheduler/dequeued/memory': 6,
	 'scheduler/enqueued': 6,
	 'scheduler/enqueued/memory': 6,
	 'spider_exceptions/TypeError': 6,
	 'start_time': datetime.datetime(2014, 10, 7, 23, 1, 53, 658000)}
2014-10-07 20:01:55-0300 [mangaupdates] INFO: Spider closed (finished)
2014-10-08 15:54:33-0300 [scrapy] INFO: Scrapy 0.24.4 started (bot: crawler)
2014-10-08 15:54:33-0300 [scrapy] INFO: Optional features available: ssl, http11
2014-10-08 15:54:33-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'crawler.spiders', 'SPIDER_MODULES': ['crawler.spiders'], 'LOG_FILE': 'logs.txt', 'BOT_NAME': 'crawler'}
2014-10-08 15:54:36-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2014-10-08 15:54:36-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2014-10-08 15:54:36-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2014-10-08 15:54:36-0300 [scrapy] INFO: Enabled item pipelines: 
2014-10-08 15:54:36-0300 [mangaupdates] INFO: Spider opened
2014-10-08 15:54:36-0300 [mangaupdates] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2014-10-08 15:54:36-0300 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2014-10-08 15:54:36-0300 [scrapy] DEBUG: Web service listening on 127.0.0.1:6080
2014-10-08 15:54:36-0300 [mangaupdates] DEBUG: Crawled (200) <GET http://www.mangaupdates.com/login.html> (referer: None)
2014-10-08 15:54:37-0300 [mangaupdates] DEBUG: Redirecting (302) to <GET http://www.mangaupdates.com/login.html> from <POST http://www.mangaupdates.com/login.html>
2014-10-08 15:54:37-0300 [mangaupdates] DEBUG: Filtered duplicate request: <GET http://www.mangaupdates.com/login.html> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2014-10-08 15:54:37-0300 [mangaupdates] INFO: Closing spider (finished)
2014-10-08 15:54:37-0300 [mangaupdates] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 655,
	 'downloader/request_count': 2,
	 'downloader/request_method_count/GET': 1,
	 'downloader/request_method_count/POST': 1,
	 'downloader/response_bytes': 3882,
	 'downloader/response_count': 2,
	 'downloader/response_status_count/200': 1,
	 'downloader/response_status_count/302': 1,
	 'dupefilter/filtered': 1,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2014, 10, 8, 18, 54, 37, 272000),
	 'log_count/DEBUG': 5,
	 'log_count/INFO': 7,
	 'request_depth_max': 1,
	 'response_received_count': 1,
	 'scheduler/dequeued': 2,
	 'scheduler/dequeued/memory': 2,
	 'scheduler/enqueued': 2,
	 'scheduler/enqueued/memory': 2,
	 'start_time': datetime.datetime(2014, 10, 8, 18, 54, 36, 276000)}
2014-10-08 15:54:37-0300 [mangaupdates] INFO: Spider closed (finished)
2014-10-08 15:55:52-0300 [scrapy] INFO: Scrapy 0.24.4 started (bot: crawler)
2014-10-08 15:55:52-0300 [scrapy] INFO: Optional features available: ssl, http11
2014-10-08 15:55:52-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'crawler.spiders', 'SPIDER_MODULES': ['crawler.spiders'], 'LOG_FILE': 'logs.txt', 'BOT_NAME': 'crawler'}
2014-10-08 15:55:53-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2014-10-08 15:55:53-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2014-10-08 15:55:53-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2014-10-08 15:55:53-0300 [scrapy] INFO: Enabled item pipelines: 
2014-10-08 15:55:53-0300 [mangaupdates] INFO: Spider opened
2014-10-08 15:55:53-0300 [mangaupdates] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2014-10-08 15:55:53-0300 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2014-10-08 15:55:53-0300 [scrapy] DEBUG: Web service listening on 127.0.0.1:6080
2014-10-08 15:55:55-0300 [mangaupdates] DEBUG: Crawled (200) <GET http://www.mangaupdates.com/login.html> (referer: None)
2014-10-08 15:55:56-0300 [mangaupdates] DEBUG: Retrying <POST http://www.mangaupdates.com/login.html> (failed 1 times): [<twisted.python.failure.Failure <class 'twisted.internet.error.ConnectionDone'>>]
2014-10-08 15:55:56-0300 [mangaupdates] DEBUG: Redirecting (302) to <GET http://www.mangaupdates.com/login.html> from <POST http://www.mangaupdates.com/login.html>
2014-10-08 15:55:57-0300 [mangaupdates] DEBUG: Crawled (200) <GET http://www.mangaupdates.com/login.html> (referer: http://www.mangaupdates.com/login.html)
2014-10-08 15:55:57-0300 [mangaupdates] DEBUG: Successfully logged in. Let's start crawling!
2014-10-08 15:55:57-0300 [mangaupdates] INFO: Closing spider (finished)
2014-10-08 15:55:57-0300 [mangaupdates] INFO: Dumping Scrapy stats:
	{'downloader/exception_count': 1,
	 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 1,
	 'downloader/request_bytes': 1432,
	 'downloader/request_count': 4,
	 'downloader/request_method_count/GET': 2,
	 'downloader/request_method_count/POST': 2,
	 'downloader/response_bytes': 7189,
	 'downloader/response_count': 3,
	 'downloader/response_status_count/200': 2,
	 'downloader/response_status_count/302': 1,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2014, 10, 8, 18, 55, 57, 236000),
	 'log_count/DEBUG': 7,
	 'log_count/INFO': 7,
	 'request_depth_max': 1,
	 'response_received_count': 2,
	 'scheduler/dequeued': 4,
	 'scheduler/dequeued/memory': 4,
	 'scheduler/enqueued': 4,
	 'scheduler/enqueued/memory': 4,
	 'start_time': datetime.datetime(2014, 10, 8, 18, 55, 53, 809000)}
2014-10-08 15:55:57-0300 [mangaupdates] INFO: Spider closed (finished)
2014-10-08 17:14:37-0300 [scrapy] INFO: Scrapy 0.24.4 started (bot: crawler)
2014-10-08 17:14:37-0300 [scrapy] INFO: Optional features available: ssl, http11
2014-10-08 17:14:37-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'crawler.spiders', 'SPIDER_MODULES': ['crawler.spiders'], 'LOG_FILE': 'logs.txt', 'BOT_NAME': 'crawler'}
2014-10-08 17:14:38-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2014-10-08 17:14:38-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2014-10-08 17:14:38-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2014-10-08 17:14:38-0300 [scrapy] INFO: Enabled item pipelines: 
2014-10-08 17:14:38-0300 [mangaupdates] INFO: Spider opened
2014-10-08 17:14:38-0300 [mangaupdates] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2014-10-08 17:14:38-0300 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2014-10-08 17:14:38-0300 [scrapy] DEBUG: Web service listening on 127.0.0.1:6080
2014-10-08 17:14:39-0300 [mangaupdates] DEBUG: Crawled (200) <GET http://www.mangaupdates.com/login.html> (referer: None)
2014-10-08 17:14:40-0300 [mangaupdates] DEBUG: Redirecting (302) to <GET http://www.mangaupdates.com/login.html> from <POST http://www.mangaupdates.com/login.html>
2014-10-08 17:14:40-0300 [mangaupdates] DEBUG: Crawled (200) <GET http://www.mangaupdates.com/login.html> (referer: http://www.mangaupdates.com/login.html)
2014-10-08 17:14:40-0300 [mangaupdates] DEBUG: Successfully logged in. Let's start crawling!
2014-10-08 17:14:40-0300 [mangaupdates] ERROR: Spider error processing <GET http://www.mangaupdates.com/login.html>
	Traceback (most recent call last):
	  File "c:\python27\lib\site-packages\twisted\internet\base.py", line 1201, in mainLoop
	    self.runUntilCurrent()
	  File "c:\python27\lib\site-packages\twisted\internet\base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "c:\python27\lib\site-packages\twisted\internet\defer.py", line 382, in callback
	    self._startRunCallbacks(result)
	  File "c:\python27\lib\site-packages\twisted\internet\defer.py", line 490, in _startRunCallbacks
	    self._runCallbacks()
	--- <exception caught here> ---
	  File "c:\python27\lib\site-packages\twisted\internet\defer.py", line 577, in _runCallbacks
	    current.result = callback(current.result, *args, **kw)
	  File "J:\Documentos\Senac\Quarto Semestre\Projeto Interativo IV\BCC-2s14-PI4-AtraxRobustus\src\crawler\crawler\spiders\mangaupdates.py", line 65, in after_login
	    return self.initialized()
	exceptions.AttributeError: 'MangaUpdatesSpider' object has no attribute 'initialized'
	
2014-10-08 17:14:40-0300 [mangaupdates] INFO: Closing spider (finished)
2014-10-08 17:14:40-0300 [mangaupdates] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 1006,
	 'downloader/request_count': 3,
	 'downloader/request_method_count/GET': 2,
	 'downloader/request_method_count/POST': 1,
	 'downloader/response_bytes': 7189,
	 'downloader/response_count': 3,
	 'downloader/response_status_count/200': 2,
	 'downloader/response_status_count/302': 1,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2014, 10, 8, 20, 14, 40, 964000),
	 'log_count/DEBUG': 6,
	 'log_count/ERROR': 1,
	 'log_count/INFO': 7,
	 'request_depth_max': 1,
	 'response_received_count': 2,
	 'scheduler/dequeued': 3,
	 'scheduler/dequeued/memory': 3,
	 'scheduler/enqueued': 3,
	 'scheduler/enqueued/memory': 3,
	 'spider_exceptions/AttributeError': 1,
	 'start_time': datetime.datetime(2014, 10, 8, 20, 14, 38, 887000)}
2014-10-08 17:14:40-0300 [mangaupdates] INFO: Spider closed (finished)
